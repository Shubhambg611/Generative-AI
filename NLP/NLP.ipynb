{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello,Welcome,to video.\n",
    "In this course! you will learn about the NLP.\n",
    "the course is very good to learn.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,Welcome,to video.\n",
      "In this course! you will learn about the NLP.\n",
      "the course is very good to learn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Tokanization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sentence --> paragraph\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gen/lib/python3.11/site-packages/nltk/downloader.py:760\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gen/lib/python3.11/site-packages/nltk/downloader.py:1110\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TKINTER:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m         \u001b[43mDownloaderGUI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TclError:\n\u001b[1;32m   1112\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/gen/lib/python3.11/site-packages/nltk/downloader.py:1933\u001b[0m, in \u001b[0;36mDownloaderGUI.mainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gen/lib/python3.11/tkinter/__init__.py:1485\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Tokanization\n",
    "# sentence --> paragraph\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,Welcome,to video.', 'In this course!', 'you will learn about the NLP.', 'the course is very good to learn.']\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paragraph --> Words\n",
    "## sentence --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Welcome', ',', 'to', 'video', '.']\n",
      "['In', 'this', 'course', '!']\n",
      "['you', 'will', 'learn', 'about', 'the', 'NLP', '.']\n",
      "['the', 'course', 'is', 'very', 'good', 'to', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treebank word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingeating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball stemmer\n",
    "better than porter stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ----------- hello\n",
      ", (punctuation, not stemmed)\n",
      "Welcome ----------- welcom\n",
      ", (punctuation, not stemmed)\n",
      "to ----------- to\n",
      "video ----------- video\n",
      ". (punctuation, not stemmed)\n",
      "In ----------- in\n",
      "this ----------- this\n",
      "course ----------- cours\n",
      "! (punctuation, not stemmed)\n",
      "you ----------- you\n",
      "will ----------- will\n",
      "learn ----------- learn\n",
      "about ----------- about\n",
      "the ----------- the\n",
      "NLP ----------- nlp\n",
      ". (punctuation, not stemmed)\n",
      "the ----------- the\n",
      "course ----------- cours\n",
      "is ----------- is\n",
      "very ----------- veri\n",
      "good ----------- good\n",
      "to ----------- to\n",
      "learn ----------- learn\n",
      ". (punctuation, not stemmed)\n",
      "eating ----------- eat\n",
      "history ----------- histori\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Define the list of words\n",
    "words = [\n",
    "    'Hello', ',', 'Welcome', ',', 'to', 'video', '.', 'In', 'this', 'course', '!',\n",
    "    'you', 'will', 'learn', 'about', 'the', 'NLP', '.', 'the', 'course', 'is',\n",
    "    'very', 'good', 'to', 'learn', '.', 'eating','history'\n",
    "]\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Iterate over each word and print the stemmed version\n",
    "for word in words:\n",
    "    # Check if the word is alphabetic to avoid stemming punctuation\n",
    "    if word.isalpha():\n",
    "        print(f\"{word} ----------- {stemmer.stem(word)}\")\n",
    "    else:\n",
    "        print(f\"{word} (punctuation, not stemmed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff between porter and snowball "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now with snowball\n",
    "snow.stem(\"fairly\"),snow.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "technique is like stemming. the output we get after lemmatization is called lemma which is a root word rather than root stem the output of stemming after lemmatization we will be getting a valid words that the same thing \n",
    "\n",
    "* POS - None-n\n",
    "* verb - v\n",
    "* adjective - a\n",
    "* adverb - r\n",
    "\n",
    "### used in Q&A chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eating\n",
      "eats------>eats\n",
      "eaten------>eaten\n",
      "writing------>writing\n",
      "writes------>writes\n",
      "programming------>programming\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemma.lemmatize(word,pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalize\n"
     ]
    }
   ],
   "source": [
    "#better result when using verb \n",
    "for word in words:\n",
    "    print(word+\"------>\"+lemma.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"fairly\",pos=\"a\"),lemma.lemmatize(\"sportingly\",pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text preprocessing stopword \n",
    "\n",
    "In Natural Language Processing (NLP), stop words are common words that are filtered out because they have little value in helping processors answer queries. Stop words are often removed as a pre-processing step in NLP applications. M<br>\n",
    "<b>Here are some examples of stop words in English:</b>\n",
    "* Articles: a, an, the\n",
    "* Conjunctions: and, but, or\n",
    "* Prepositions: in, on, at, with\n",
    "* Pronouns: he, she, it, they\n",
    "* Common verbs: is, am, are, was, were, be, being, been\n",
    "\n",
    "Removing stop words can improve the efficiency and accuracy of text mining applications. It can also reduce the size of the corpus by 35–45%. \n",
    "There is no single universal list of stop words used by all NLP tools. The specific stop words can vary based on context and purpose. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"I am indeed delighted to be here with the Future of India on this Children's Day, 14th November, 2002. My greetings to the children, teachers and members of Industrial community particularly members of CII who have organized this event and to release this vision document \"Developed India : Mission for the Young\". I am happy to inform you that today I have completed my mission of interacting with 100,000 school children across the length and breadth of the nation. Wherever I went, be it Arunachal Pradesh, Nagaland, Madhya Pradesh, Gujarat, Karnataka or any other part of India, the voice of the youth is unique and strong in articulating their vision and dream. Everyone dreams of living in a prosperous India, a happy India and a peaceful India. The combination of prosperity, happiness and peace to a nation always come together. When all three of them converge on to India, then India truly be a Developed Nation. Today I am going to talk to you about how India can be transformed into a Developed Nation. There are more than 300 million children of your age group in India. There are about 2000 children in this hall. What can this smaller fraction do to realize the dream of Developed India.\n",
    "\n",
    "Even though you are only a very small speck of ocean, of the 300 million children on whose shoulders the future of India rests, you can ignite the minds, light the fire, become a burning candle to light another candle. My dear children, you work in the school for about 25000 hours before you complete 12th std. Within the school curriculum / school hours, I want you to contribute to the Mission of Developed India by involving in the student centric activities like Literacy Development, Eco-Care Movement.\n",
    "\n",
    "India after its independence was determined to move ahead with planned policies for Science & Technology. Now, India is very near to self-sufficiency in food, making the ship to mouth existence of 1950s, an event of the past. Also improvements in the health sector, have eliminated few contagious diseases. There is a increase in life expectancy. Small scale industries provide high percentage of National GDP - a vast change in 1990s compared to 1950s. Today India can design, develop and launch world class geo-stationary and sun synchronous, remote sensing satellites.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shubham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we will apply stemming and remove the stopword from the para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(para)\n",
    "# every sentence is seperated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and fillter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i inde delight futur india children 's day , 14th novemb , 2002 .\",\n",
       " \"my greet children , teacher member industri commun particularli member cii organ event releas vision document `` develop india : mission young '' .\",\n",
       " 'i happi inform today i complet mission interact 100,000 school children across length breadth nation .',\n",
       " 'wherev i went , arunach pradesh , nagaland , madhya pradesh , gujarat , karnataka part india , voic youth uniqu strong articul vision dream .',\n",
       " 'everyon dream live prosper india , happi india peac india .',\n",
       " 'the combin prosper , happi peac nation alway come togeth .',\n",
       " 'when three converg india , india truli develop nation .',\n",
       " 'today i go talk india transform develop nation .',\n",
       " 'there 300 million children age group india .',\n",
       " 'there 2000 children hall .',\n",
       " 'what smaller fraction realiz dream develop india .',\n",
       " 'even though small speck ocean , 300 million children whose shoulder futur india rest , ignit mind , light fire , becom burn candl light anoth candl .',\n",
       " 'my dear children , work school 25000 hour complet 12th std .',\n",
       " 'within school curriculum / school hour , i want contribut mission develop india involv student centric activ like literaci develop , eco-car movement .',\n",
       " 'india independ determin move ahead plan polici scienc & technolog .',\n",
       " 'now , india near self-suffici food , make ship mouth exist 1950 , event past .',\n",
       " 'also improv health sector , elimin contagi diseas .',\n",
       " 'there increas life expect .',\n",
       " 'small scale industri provid high percentag nation gdp - vast chang 1990 compar 1950 .',\n",
       " 'today india design , develop launch world class geo-stationari sun synchron , remot sens satellit .']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'india',\n",
       " 'design',\n",
       " ',',\n",
       " 'develop',\n",
       " 'launch',\n",
       " 'world',\n",
       " 'class',\n",
       " 'geo-stationari',\n",
       " 'sun',\n",
       " 'synchron',\n",
       " ',',\n",
       " 'remot',\n",
       " 'sens',\n",
       " 'satellit',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and fillter and then apply snowballstemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [snow.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ind delight futur india children 's day , 14th novemb , 2002 .\",\n",
       " 'greet children , teacher member industri commun particular member cii organ event relea vision document `` develop india : mission young `` .',\n",
       " 'happi inform today complet mission interact 100,000 school children across length breadth nation .',\n",
       " 'wherev went , arunach pradesh , nagaland , madhya pradesh , gujarat , karnataka part india , voic youth uniqu strong articul vision dream .',\n",
       " 'everyon dream live prosper india , happi india peac india .',\n",
       " 'combin prosper , happi peac nation alway come togeth .',\n",
       " 'three converg india , india truli develop nation .',\n",
       " 'today go talk india transform develop nation .',\n",
       " '300 million children age group india .',\n",
       " '2000 children hall .',\n",
       " 'smaller fraction realiz dream develop india .',\n",
       " 'even though small speck ocean , 300 million children whose shoulder futur india rest , ignit mind , light fire , becom burn candl light anoth candl .',\n",
       " 'dear children , work school 25000 hour complet 12th std .',\n",
       " 'within school curriculum / school hour , want contribut mission develop india involv student centric activ like literaci develop , eco-car movement .',\n",
       " 'india independ determin move ahead plan polici scienc & technolog .',\n",
       " ', india near self-suffici food , make ship mouth exist 1950 , event past .',\n",
       " 'also improv health sector , elimin contagi disea .',\n",
       " 'increa life expect .',\n",
       " 'small scale industri provid high percentag nation gdp - vast chang 1990 compar 1950 .',\n",
       " 'today india design , develop launch world class geo-stationari sun synchron , remot sen satellit .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now apply lemmatization\n",
    "## Apply stopwords and fillter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [lemma.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]   #.lower used to convert all the sentences in lowercase\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ind delight futur india children 's day , 14th novemb , 2002 .\",\n",
       " 'greet children , teacher member industri commun particular member cii organ event relea vision document `` develop india : mission young `` .',\n",
       " 'happi inform today complet mission interact 100,000 school children across length breadth nation .',\n",
       " 'wherev go , arunach pradesh , nagaland , madhya pradesh , gujarat , karnataka part india , voic youth uniqu strong articul vision dream .',\n",
       " 'everyon dream live prosper india , happi india peac india .',\n",
       " 'combin prosper , happi peac nation alway come togeth .',\n",
       " 'three converg india , india truli develop nation .',\n",
       " 'today go talk india transform develop nation .',\n",
       " '300 million children age group india .',\n",
       " '2000 children hall .',\n",
       " 'smaller fraction realiz dream develop india .',\n",
       " 'even though small speck ocean , 300 million children whose shoulder futur india rest , ignit mind , light fire , becom burn candl light anoth candl .',\n",
       " 'dear children , work school 25000 hour complet 12th std .',\n",
       " 'within school curriculum / school hour , want contribut mission develop india involv student centric activ like literaci develop , eco-car movement .',\n",
       " 'india independ determin move ahead plan polici scienc & technolog .',\n",
       " ', india near self-suffici food , make ship mouth exist 1950 , event past .',\n",
       " 'also improv health sector , elimin contagi disea .',\n",
       " 'increa life expect .',\n",
       " 'small scale industri provid high percentag nation gdp - vast chang 1990 compar 1950 .',\n",
       " 'today india design , develop launch world class geo-stationari sun synchron , remot sen satellit .']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vid 42  POS tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am indeed delighted to be here with the Future of India on this Children\\'s Day, 14th November, 2002. My greetings to the children, teachers and members of Industrial community particularly members of CII who have organized this event and to release this vision document \"Developed India : Mission for the Young\". I am happy to inform you that today I have completed my mission of interacting with 100,000 school children across the length and breadth of the nation. Wherever I went, be it Arunachal Pradesh, Nagaland, Madhya Pradesh, Gujarat, Karnataka or any other part of India, the voice of the youth is unique and strong in articulating their vision and dream. Everyone dreams of living in a prosperous India, a happy India and a peaceful India. The combination of prosperity, happiness and peace to a nation always come together. When all three of them converge on to India, then India truly be a Developed Nation. Today I am going to talk to you about how India can be transformed into a Developed Nation. There are more than 300 million children of your age group in India. There are about 2000 children in this hall. What can this smaller fraction do to realize the dream of Developed India.\\n\\nEven though you are only a very small speck of ocean, of the 300 million children on whose shoulders the future of India rests, you can ignite the minds, light the fire, become a burning candle to light another candle. My dear children, you work in the school for about 25000 hours before you complete 12th std. Within the school curriculum / school hours, I want you to contribute to the Mission of Developed India by involving in the student centric activities like Literacy Development, Eco-Care Movement.\\n\\nIndia after its independence was determined to move ahead with planned policies for Science & Technology. Now, India is very near to self-sufficiency in food, making the ship to mouth existence of 1950s, an event of the past. Also improvements in the health sector, have eliminated few contagious diseases. There is a increase in life expectancy. Small scale industries provide high percentage of National GDP - a vast change in 1990s compared to 1950s. Today India can design, develop and launch world class geo-stationary and sun synchronous, remote sensing satellites.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sentences=nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I am indeed delighted to be here with the Future of India on this Children's Day, 14th November, 2002.\",\n",
       " 'My greetings to the children, teachers and members of Industrial community particularly members of CII who have organized this event and to release this vision document \"Developed India : Mission for the Young\".',\n",
       " 'I am happy to inform you that today I have completed my mission of interacting with 100,000 school children across the length and breadth of the nation.',\n",
       " 'Wherever I went, be it Arunachal Pradesh, Nagaland, Madhya Pradesh, Gujarat, Karnataka or any other part of India, the voice of the youth is unique and strong in articulating their vision and dream.',\n",
       " 'Everyone dreams of living in a prosperous India, a happy India and a peaceful India.',\n",
       " 'The combination of prosperity, happiness and peace to a nation always come together.',\n",
       " 'When all three of them converge on to India, then India truly be a Developed Nation.',\n",
       " 'Today I am going to talk to you about how India can be transformed into a Developed Nation.',\n",
       " 'There are more than 300 million children of your age group in India.',\n",
       " 'There are about 2000 children in this hall.',\n",
       " 'What can this smaller fraction do to realize the dream of Developed India.',\n",
       " 'Even though you are only a very small speck of ocean, of the 300 million children on whose shoulders the future of India rests, you can ignite the minds, light the fire, become a burning candle to light another candle.',\n",
       " 'My dear children, you work in the school for about 25000 hours before you complete 12th std.',\n",
       " 'Within the school curriculum / school hours, I want you to contribute to the Mission of Developed India by involving in the student centric activities like Literacy Development, Eco-Care Movement.',\n",
       " 'India after its independence was determined to move ahead with planned policies for Science & Technology.',\n",
       " 'Now, India is very near to self-sufficiency in food, making the ship to mouth existence of 1950s, an event of the past.',\n",
       " 'Also improvements in the health sector, have eliminated few contagious diseases.',\n",
       " 'There is a increase in life expectancy.',\n",
       " 'Small scale industries provide high percentage of National GDP - a vast change in 1990s compared to 1950s.',\n",
       " 'Today India can design, develop and launch world class geo-stationary and sun synchronous, remote sensing satellites.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('indeed', 'RB'), ('delighted', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('here', 'RB'), ('with', 'IN'), ('the', 'DT'), ('Future', 'NN'), ('of', 'IN'), ('India', 'NNP'), ('on', 'IN'), ('this', 'DT'), ('Children', 'NNP'), (\"'s\", 'POS'), ('Day', 'NNP'), (',', ','), ('14th', 'CD'), ('November', 'NNP'), (',', ','), ('2002', 'CD'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('greetings', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('children', 'NNS'), (',', ','), ('teachers', 'NNS'), ('and', 'CC'), ('members', 'NNS'), ('of', 'IN'), ('Industrial', 'NNP'), ('community', 'NN'), ('particularly', 'RB'), ('members', 'NNS'), ('of', 'IN'), ('CII', 'NNP'), ('who', 'WP'), ('have', 'VBP'), ('organized', 'VBN'), ('this', 'DT'), ('event', 'NN'), ('and', 'CC'), ('to', 'TO'), ('release', 'VB'), ('this', 'DT'), ('vision', 'NN'), ('document', 'NN'), ('``', '``'), ('Developed', 'NNP'), ('India', 'NNP'), (':', ':'), ('Mission', 'NN'), ('for', 'IN'), ('the', 'DT'), ('Young', 'NNP'), ('``', '``'), ('.', '.')]\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('happy', 'JJ'), ('to', 'TO'), ('inform', 'VB'), ('you', 'PRP'), ('that', 'IN'), ('today', 'NN'), ('I', 'PRP'), ('have', 'VBP'), ('completed', 'VBN'), ('my', 'PRP$'), ('mission', 'NN'), ('of', 'IN'), ('interacting', 'VBG'), ('with', 'IN'), ('100,000', 'CD'), ('school', 'NN'), ('children', 'NNS'), ('across', 'IN'), ('the', 'DT'), ('length', 'NN'), ('and', 'CC'), ('breadth', 'NN'), ('of', 'IN'), ('the', 'DT'), ('nation', 'NN'), ('.', '.')]\n",
      "[('Wherever', 'WRB'), ('I', 'PRP'), ('went', 'VBD'), (',', ','), ('be', 'VB'), ('it', 'PRP'), ('Arunachal', 'NNP'), ('Pradesh', 'NNP'), (',', ','), ('Nagaland', 'NNP'), (',', ','), ('Madhya', 'NNP'), ('Pradesh', 'NNP'), (',', ','), ('Gujarat', 'NNP'), (',', ','), ('Karnataka', 'NNP'), ('or', 'CC'), ('any', 'DT'), ('other', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('India', 'NNP'), (',', ','), ('the', 'DT'), ('voice', 'NN'), ('of', 'IN'), ('the', 'DT'), ('youth', 'NN'), ('is', 'VBZ'), ('unique', 'JJ'), ('and', 'CC'), ('strong', 'JJ'), ('in', 'IN'), ('articulating', 'VBG'), ('their', 'PRP$'), ('vision', 'NN'), ('and', 'CC'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Everyone', 'NN'), ('dreams', 'NN'), ('of', 'IN'), ('living', 'NN'), ('in', 'IN'), ('a', 'DT'), ('prosperous', 'JJ'), ('India', 'NNP'), (',', ','), ('a', 'DT'), ('happy', 'JJ'), ('India', 'NNP'), ('and', 'CC'), ('a', 'DT'), ('peaceful', 'JJ'), ('India', 'NNP'), ('.', '.')]\n",
      "[('The', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('prosperity', 'NN'), (',', ','), ('happiness', 'NN'), ('and', 'CC'), ('peace', 'NN'), ('to', 'TO'), ('a', 'DT'), ('nation', 'NN'), ('always', 'RB'), ('come', 'VBN'), ('together', 'RB'), ('.', '.')]\n",
      "[('When', 'WRB'), ('all', 'DT'), ('three', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('converge', 'VBP'), ('on', 'IN'), ('to', 'TO'), ('India', 'NNP'), (',', ','), ('then', 'RB'), ('India', 'NNP'), ('truly', 'RB'), ('be', 'VB'), ('a', 'DT'), ('Developed', 'NNP'), ('Nation', 'NNP'), ('.', '.')]\n",
      "[('Today', 'NN'), ('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('talk', 'VB'), ('to', 'TO'), ('you', 'PRP'), ('about', 'IN'), ('how', 'WRB'), ('India', 'NNP'), ('can', 'MD'), ('be', 'VB'), ('transformed', 'VBN'), ('into', 'IN'), ('a', 'DT'), ('Developed', 'NNP'), ('Nation', 'NNP'), ('.', '.')]\n",
      "[('There', 'EX'), ('are', 'VBP'), ('more', 'JJR'), ('than', 'IN'), ('300', 'CD'), ('million', 'CD'), ('children', 'NNS'), ('of', 'IN'), ('your', 'PRP$'), ('age', 'NN'), ('group', 'NN'), ('in', 'IN'), ('India', 'NNP'), ('.', '.')]\n",
      "[('There', 'EX'), ('are', 'VBP'), ('about', 'IN'), ('2000', 'CD'), ('children', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('hall', 'NN'), ('.', '.')]\n",
      "[('What', 'WP'), ('can', 'MD'), ('this', 'DT'), ('smaller', 'JJR'), ('fraction', 'NN'), ('do', 'VBP'), ('to', 'TO'), ('realize', 'VB'), ('the', 'DT'), ('dream', 'NN'), ('of', 'IN'), ('Developed', 'NNP'), ('India', 'NNP'), ('.', '.')]\n",
      "[('Even', 'RB'), ('though', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('only', 'RB'), ('a', 'DT'), ('very', 'RB'), ('small', 'JJ'), ('speck', 'NN'), ('of', 'IN'), ('ocean', 'JJ'), (',', ','), ('of', 'IN'), ('the', 'DT'), ('300', 'CD'), ('million', 'CD'), ('children', 'NNS'), ('on', 'IN'), ('whose', 'WP$'), ('shoulders', 'NNS'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('India', 'NNP'), ('rests', 'NNS'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('ignite', 'VB'), ('the', 'DT'), ('minds', 'NNS'), (',', ','), ('light', 'VBD'), ('the', 'DT'), ('fire', 'NN'), (',', ','), ('become', 'VB'), ('a', 'DT'), ('burning', 'VBG'), ('candle', 'NN'), ('to', 'TO'), ('light', 'VB'), ('another', 'DT'), ('candle', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('dear', 'JJ'), ('children', 'NNS'), (',', ','), ('you', 'PRP'), ('work', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('school', 'NN'), ('for', 'IN'), ('about', 'IN'), ('25000', 'CD'), ('hours', 'NNS'), ('before', 'IN'), ('you', 'PRP'), ('complete', 'VBP'), ('12th', 'CD'), ('std', 'NN'), ('.', '.')]\n",
      "[('Within', 'IN'), ('the', 'DT'), ('school', 'NN'), ('curriculum', 'NN'), ('/', 'NNP'), ('school', 'NN'), ('hours', 'NNS'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('you', 'PRP'), ('to', 'TO'), ('contribute', 'VB'), ('to', 'TO'), ('the', 'DT'), ('Mission', 'NNP'), ('of', 'IN'), ('Developed', 'NNP'), ('India', 'NNP'), ('by', 'IN'), ('involving', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('student', 'NN'), ('centric', 'JJ'), ('activities', 'NNS'), ('like', 'IN'), ('Literacy', 'NNP'), ('Development', 'NNP'), (',', ','), ('Eco-Care', 'NNP'), ('Movement', 'NNP'), ('.', '.')]\n",
      "[('India', 'NNP'), ('after', 'IN'), ('its', 'PRP$'), ('independence', 'NN'), ('was', 'VBD'), ('determined', 'VBN'), ('to', 'TO'), ('move', 'VB'), ('ahead', 'RB'), ('with', 'IN'), ('planned', 'JJ'), ('policies', 'NNS'), ('for', 'IN'), ('Science', 'NNP'), ('&', 'CC'), ('Technology', 'NNP'), ('.', '.')]\n",
      "[('Now', 'RB'), (',', ','), ('India', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('near', 'JJ'), ('to', 'TO'), ('self-sufficiency', 'NN'), ('in', 'IN'), ('food', 'NN'), (',', ','), ('making', 'VBG'), ('the', 'DT'), ('ship', 'NN'), ('to', 'TO'), ('mouth', 'VB'), ('existence', 'NN'), ('of', 'IN'), ('1950s', 'CD'), (',', ','), ('an', 'DT'), ('event', 'NN'), ('of', 'IN'), ('the', 'DT'), ('past', 'NN'), ('.', '.')]\n",
      "[('Also', 'RB'), ('improvements', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('health', 'NN'), ('sector', 'NN'), (',', ','), ('have', 'VBP'), ('eliminated', 'VBN'), ('few', 'JJ'), ('contagious', 'JJ'), ('diseases', 'NNS'), ('.', '.')]\n",
      "[('There', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('increase', 'NN'), ('in', 'IN'), ('life', 'NN'), ('expectancy', 'NN'), ('.', '.')]\n",
      "[('Small', 'NNP'), ('scale', 'NN'), ('industries', 'NNS'), ('provide', 'VBP'), ('high', 'JJ'), ('percentage', 'NN'), ('of', 'IN'), ('National', 'NNP'), ('GDP', 'NNP'), ('-', ':'), ('a', 'DT'), ('vast', 'JJ'), ('change', 'NN'), ('in', 'IN'), ('1990s', 'CD'), ('compared', 'VBN'), ('to', 'TO'), ('1950s', 'CD'), ('.', '.')]\n",
      "[('Today', 'NN'), ('India', 'NNP'), ('can', 'MD'), ('design', 'VB'), (',', ','), ('develop', 'VB'), ('and', 'CC'), ('launch', 'JJ'), ('world', 'NN'), ('class', 'NN'), ('geo-stationary', 'JJ'), ('and', 'CC'), ('sun', 'JJ'), ('synchronous', 'JJ'), (',', ','), ('remote', 'JJ'), ('sensing', 'VBG'), ('satellites', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tag\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    word = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
