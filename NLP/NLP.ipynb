{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello,Welcome,to video.\n",
    "In this course! you will learn about the NLP.\n",
    "the course is very good to learn.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,Welcome,to video.\n",
      "In this course! you will learn about the NLP.\n",
      "the course is very good to learn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokanization\n",
    "# sentence --> paragraph\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,Welcome,to video.', 'In this course!', 'you will learn about the NLP.', 'the course is very good to learn.']\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paragraph --> Words\n",
    "## sentence --> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Welcome', ',', 'to', 'video', '.']\n",
      "['In', 'this', 'course', '!']\n",
      "['you', 'will', 'learn', 'about', 'the', 'NLP', '.']\n",
      "['the', 'course', 'is', 'very', 'good', 'to', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treebank word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'video.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'course',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'the',\n",
       " 'NLP.',\n",
       " 'the',\n",
       " 'course',\n",
       " 'is',\n",
       " 'very',\n",
       " 'good',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingeating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball stemmer\n",
    "better than porter stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ----------- hello\n",
      ", (punctuation, not stemmed)\n",
      "Welcome ----------- welcom\n",
      ", (punctuation, not stemmed)\n",
      "to ----------- to\n",
      "video ----------- video\n",
      ". (punctuation, not stemmed)\n",
      "In ----------- in\n",
      "this ----------- this\n",
      "course ----------- cours\n",
      "! (punctuation, not stemmed)\n",
      "you ----------- you\n",
      "will ----------- will\n",
      "learn ----------- learn\n",
      "about ----------- about\n",
      "the ----------- the\n",
      "NLP ----------- nlp\n",
      ". (punctuation, not stemmed)\n",
      "the ----------- the\n",
      "course ----------- cours\n",
      "is ----------- is\n",
      "very ----------- veri\n",
      "good ----------- good\n",
      "to ----------- to\n",
      "learn ----------- learn\n",
      ". (punctuation, not stemmed)\n",
      "eating ----------- eat\n",
      "history ----------- histori\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Define the list of words\n",
    "words = [\n",
    "    'Hello', ',', 'Welcome', ',', 'to', 'video', '.', 'In', 'this', 'course', '!',\n",
    "    'you', 'will', 'learn', 'about', 'the', 'NLP', '.', 'the', 'course', 'is',\n",
    "    'very', 'good', 'to', 'learn', '.', 'eating','history'\n",
    "]\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Iterate over each word and print the stemmed version\n",
    "for word in words:\n",
    "    # Check if the word is alphabetic to avoid stemming punctuation\n",
    "    if word.isalpha():\n",
    "        print(f\"{word} ----------- {stemmer.stem(word)}\")\n",
    "    else:\n",
    "        print(f\"{word} (punctuation, not stemmed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff between porter and snowball "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now with snowball\n",
    "snow.stem(\"fairly\"),snow.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "technique is like stemming. the output we get after lemmatization is called lemma which is a root word rather than root stem the output of stemming after lemmatization we will be getting a valid words that the same thing \n",
    "\n",
    "* POS - None-n\n",
    "* verb - v\n",
    "* adjective - a\n",
    "* adverb - r\n",
    "\n",
    "### used in Q&A chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eating\n",
      "eats------>eats\n",
      "eaten------>eaten\n",
      "writing------>writing\n",
      "writes------>writes\n",
      "programming------>programming\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemma.lemmatize(word,pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalize\n"
     ]
    }
   ],
   "source": [
    "#better result when using verb \n",
    "for word in words:\n",
    "    print(word+\"------>\"+lemma.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"fairly\",pos=\"a\"),lemma.lemmatize(\"sportingly\",pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text preprocessing stopword \n",
    "\n",
    "In Natural Language Processing (NLP), stop words are common words that are filtered out because they have little value in helping processors answer queries. Stop words are often removed as a pre-processing step in NLP applications. M<br>\n",
    "<b>Here are some examples of stop words in English:</b>\n",
    "* Articles: a, an, the\n",
    "* Conjunctions: and, but, or\n",
    "* Prepositions: in, on, at, with\n",
    "* Pronouns: he, she, it, they\n",
    "* Common verbs: is, am, are, was, were, be, being, been\n",
    "\n",
    "Removing stop words can improve the efficiency and accuracy of text mining applications. It can also reduce the size of the corpus by 35–45%. \n",
    "There is no single universal list of stop words used by all NLP tools. The specific stop words can vary based on context and purpose. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"I am indeed delighted to be here with the Future of India on this Children's Day, 14th November, 2002. My greetings to the children, teachers and members of Industrial community particularly members of CII who have organized this event and to release this vision document \"Developed India : Mission for the Young\". I am happy to inform you that today I have completed my mission of interacting with 100,000 school children across the length and breadth of the nation. Wherever I went, be it Arunachal Pradesh, Nagaland, Madhya Pradesh, Gujarat, Karnataka or any other part of India, the voice of the youth is unique and strong in articulating their vision and dream. Everyone dreams of living in a prosperous India, a happy India and a peaceful India. The combination of prosperity, happiness and peace to a nation always come together. When all three of them converge on to India, then India truly be a Developed Nation. Today I am going to talk to you about how India can be transformed into a Developed Nation. There are more than 300 million children of your age group in India. There are about 2000 children in this hall. What can this smaller fraction do to realize the dream of Developed India.\n",
    "\n",
    "Even though you are only a very small speck of ocean, of the 300 million children on whose shoulders the future of India rests, you can ignite the minds, light the fire, become a burning candle to light another candle. My dear children, you work in the school for about 25000 hours before you complete 12th std. Within the school curriculum / school hours, I want you to contribute to the Mission of Developed India by involving in the student centric activities like Literacy Development, Eco-Care Movement.\n",
    "\n",
    "India after its independence was determined to move ahead with planned policies for Science & Technology. Now, India is very near to self-sufficiency in food, making the ship to mouth existence of 1950s, an event of the past. Also improvements in the health sector, have eliminated few contagious diseases. There is a increase in life expectancy. Small scale industries provide high percentage of National GDP - a vast change in 1990s compared to 1950s. Today India can design, develop and launch world class geo-stationary and sun synchronous, remote sensing satellites.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shubham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we will apply stemming and remove the stopword from the para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(para)\n",
    "# every sentence is seperated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and fillter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ind delight futur india children 's day , 14th novemb , 2002 .\",\n",
       " 'greet children , teacher member industri commun particularli member cii organ event relea vision document `` develop india : mission young `` .',\n",
       " 'happi inform today complet mission interact 100,000 school children across length breadth nation .',\n",
       " 'wherev went , arunach pradesh , nagaland , madhya pradesh , gujarat , karnataka part india , voic youth uniqu strong articul vision dream .',\n",
       " 'everyon dream live prosper india , happi india peac india .',\n",
       " 'combin prosper , happi peac nation alway come togeth .',\n",
       " 'three converg india , india truli develop nation .',\n",
       " 'today go talk india transform develop nation .',\n",
       " '300 million children age group india .',\n",
       " '2000 children hall .',\n",
       " 'smaller fraction realiz dream develop india .',\n",
       " 'even though small speck ocean , 300 million children whose shoulder futur india rest , ignit mind , light fire , becom burn candl light anoth candl .',\n",
       " 'dear children , work school 25000 hour complet 12th std .',\n",
       " 'within school curriculum / school hour , want contribut mission develop india involv student centric activ like literaci develop , eco-car movement .',\n",
       " 'india independ determin move ahead plan polici scienc & technolog .',\n",
       " ', india near self-suffici food , make ship mouth exist 1950 , event past .',\n",
       " 'also improv health sector , elimin contagi disea .',\n",
       " 'increa life expect .',\n",
       " 'small scale industri provid high percentag nation gdp - vast chang 1990 compar 1950 .',\n",
       " 'today india design , develop launch world class geo-stationari sun synchron , remot sen satellit .']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'india',\n",
       " 'design',\n",
       " ',',\n",
       " 'develop',\n",
       " 'launch',\n",
       " 'world',\n",
       " 'class',\n",
       " 'geo-stationari',\n",
       " 'sun',\n",
       " 'synchron',\n",
       " ',',\n",
       " 'remot',\n",
       " 'sen',\n",
       " 'satellit',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and fillter and then apply snowballstemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [snow.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i inde delight futur india children 's day , 14th novemb , 2002 .\",\n",
       " \"my greet children , teacher member industri communiti particular member cii organ event releas vision document `` develop india : mission young '' .\",\n",
       " 'i happi inform today i complet mission interact 100,000 school children across length breadth nation .',\n",
       " 'wherev i went , arunach pradesh , nagaland , madhya pradesh , gujarat , karnataka part india , voic youth uniqu strong articul vision dream .',\n",
       " 'everyon dream live prosper india , happi india peac india .',\n",
       " 'the combin prosper , happi peac nation alway come togeth .',\n",
       " 'when three converg india , india truli develop nation .',\n",
       " 'today i go talk india transform develop nation .',\n",
       " 'there 300 million children age group india .',\n",
       " 'there 2000 children hall .',\n",
       " 'what smaller fraction realiz dream develop india .',\n",
       " 'even though small speck ocean , 300 million children whose shoulder futur india rest , ignit mind , light fire , becom burn candl light anoth candl .',\n",
       " 'my dear children , work school 25000 hour complet 12th std .',\n",
       " 'within school curriculum / school hour , i want contribut mission develop india involv student centric activ like literaci develop , eco-car movement .',\n",
       " 'india independ determin move ahead plan polici scienc & technolog .',\n",
       " 'now , india near self-suffici food , make ship mouth exist 1950s , event past .',\n",
       " 'also improv health sector , elimin contagi diseas .',\n",
       " 'there increas life expect .',\n",
       " 'small scale industri provid high percentag nation gdp - vast chang 1990s compar 1950s .',\n",
       " 'today india design , develop launch world class geo-stationari sun synchron , remot sens satellit .']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now apply lemmatization\n",
    "## Apply stopwords and fillter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    words = [lemma.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) # converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I indeed delight Future India Children 's Day , 14th November , 2002 .\",\n",
       " 'My greet child , teacher member Industrial community particularly member CII organize event release vision document `` Developed India : Mission Young `` .',\n",
       " 'I happy inform today I complete mission interact 100,000 school child across length breadth nation .',\n",
       " 'Wherever I go , Arunachal Pradesh , Nagaland , Madhya Pradesh , Gujarat , Karnataka part India , voice youth unique strong articulate vision dream .',\n",
       " 'Everyone dream live prosperous India , happy India peaceful India .',\n",
       " 'The combination prosperity , happiness peace nation always come together .',\n",
       " 'When three converge India , India truly Developed Nation .',\n",
       " 'Today I go talk India transform Developed Nation .',\n",
       " 'There 300 million child age group India .',\n",
       " 'There 2000 child hall .',\n",
       " 'What smaller fraction realize dream Developed India .',\n",
       " 'Even though small speck ocean , 300 million child whose shoulder future India rest , ignite mind , light fire , become burn candle light another candle .',\n",
       " 'My dear child , work school 25000 hour complete 12th std .',\n",
       " 'Within school curriculum / school hour , I want contribute Mission Developed India involve student centric activity like Literacy Development , Eco-Care Movement .',\n",
       " 'India independence determine move ahead plan policy Science & Technology .',\n",
       " 'Now , India near self-sufficiency food , make ship mouth existence 1950s , event past .',\n",
       " 'Also improvement health sector , eliminate contagious disease .',\n",
       " 'There increase life expectancy .',\n",
       " 'Small scale industry provide high percentage National GDP - vast change 1990s compare 1950s .',\n",
       " 'Today India design , develop launch world class geo-stationary sun synchronous , remote sense satellite .']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
